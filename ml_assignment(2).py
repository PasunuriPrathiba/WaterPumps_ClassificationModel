# -*- coding: utf-8 -*-
"""ML_Assignment(2)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GdgX5A1O5W697dCJBynT4WhXRAnRq8z_

Problem:-The ministry of water of an African country has made a data set of pumps
installed in various places in the country to serve the water for their countrymen. It is a
tedious and expensive task to maintain these pumps. This data set contains information
such as the kind of pump, when it was installed, and how it is managed. Can you predict
which pumps require repairs and which are not functional by using the given data set? A
smart understanding of water point failure can improve maintenance operations and ensure
that clean and safe water is available to these communities. Experiment with a few methods
such as tree based methods, bagging, boosting method, support vector machine etc. and
comment on the suitability of the method to predict the classes. Before implementing any
model, visualize the data and comment on the data statistics.

1) Data exploration and visualization with insightful commentary to determine potentially
useful variables
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing the required libraries

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
# %matplotlib inline

from sklearn.metrics import accuracy_score

#Reading the files into train_df and test_df
train_df = pd.read_csv('/content/sample_data/Train.csv')
test_df = pd.read_csv('/content/sample_data/test.csv')

"""Descrptive Statistics"""

train_df.head()

train_df.tail()

train_df.describe()

train_df.info()

train_df.shape

"""The train data set has 44550 observations and 41 columns.

The "status_group" shows the label or target for each pump.The other 40 columns are features, out of which 10 are numerical and 30 are categorical variables

Preliminary accuracy score
"""

label_dic = {"functional" : 2, "functional needs repair" : 1, "non functional" : 0}
train_df['label'] = train_df['status_group'].map(label_dic)
sns.distplot(train_df['label'], kde = True)

major_cls = train_df['status_group'].mode()[0]
print("The most frequent label is", major_cls)

y_prilem_pred = np.full(shape=train_df["status_group"].shape, fill_value=major_cls)
accuracy_score(train_df["status_group"],y_prilem_pred)

"""It means that we can make a preliminary estimate of 54.31% chance of a random pump from this database to be functional. This number will be a baseline for the future model predictions."""

#select numeriacl columns
num_vars = [col for col in train_df.columns if train_df[col].dtype in ['int64','float64']]
print(num_vars)

# Plotting construction_year vs status_group 
fig_dims = (15,15)
fig, ax = plt.subplots(figsize=fig_dims)


sns.countplot(x=train_df["construction_year"], hue=train_df["status_group"])




plt.xticks(rotation=45, horizontalalignment='right')

"""We can see that most pumps that were built between 1970 to 1985 are non functional, whereas the more recent pumps tend to be functional. It means that the "construction_year" feature could be very useful in our prediction model. The number of pumps that needs repair seems not very high and quite stable over the years. The rows with 0 construction year need to be checked."""

#Scatter plot of amount_tsh vs status_group
plt.scatter(y=train_df["amount_tsh"], x=train_df["status_group"])

"""If the "amount_tsh" > 200000 then most likely the pump is functional."""

#Finding the numerical variables distributions
fig = plt.figure(figsize=(16,18))
sns.distributions._has_statsmodels=False
for i in range(len(num_vars)):
    fig.add_subplot(9,4,i+1)
    sns.distplot(train_df[num_vars].iloc[:,i].dropna())
    plt.xlabel(num_vars[i])

plt.tight_layout()
plt.show()

"""Uni-modal, skewed distributions could potentially be log transformed:

GPS_hight,  
Longtitude,              
Region_code,  
District_code,

Finding outliers
"""

#Univariate analysis-Box plots for numerical variables
fig = plt.figure(figsize=(16, 18))

for i in range(len(num_vars)):
    fig.add_subplot(9, 4, i+1)
    sns.boxplot(y=train_df[num_vars].iloc[:,i])

plt.tight_layout()
plt.show()

#Bivariate analysis-Scatter plots target vs numerical variables
f = plt.figure(figsize=(14,20))

for i in range(len(num_vars)):
    f.add_subplot(9, 4, i+1)
    sns.scatterplot(train_df[num_vars].iloc[:,i], train_df["label"])
    
plt.tight_layout()
plt.show()

"""Finding correlation among variables"""

correlation = train_df.corr()

f, ax = plt.subplots(figsize=(8,6))
plt.title('Correlation among variables', size=12)
sns.heatmap(correlation,xticklabels=correlation.columns, yticklabels=correlation.columns, annot=True)

"""The correlation between "district_code" and "region_code" is quite high. Consider removing one of them.

The correlation between "construction_year" and "gps_height" is also high, but these 2 variables don't have any obvious connection, so explore this correlation further to take a decision.

With reference to the target Label, the top correlated attributes are:
"""

correlation['label'].sort_values(ascending=False)

"""The negative correlation to the target variable of the "region_code" is higher than that of the "district_code". Keep the variable with higher correlation to the target.

Linear correlation to the target is quite low for all variables but it might mean that there exists a non-linear correlation instead.

Missing/null values in numerical columns
"""

train_df[num_vars].isnull().sum().sort_values(ascending=False)

len(train_df.population[train_df.population == 0])

"""Categorical columns:-"""

cat_vars = train_df.select_dtypes(include='object').columns
print(cat_vars)

"""Missing/null values in categorical columns"""

train_df[cat_vars].isnull().sum().sort_values(ascending=False)

"""2) Variable transformation, feature engineering, feature selection or elimination."""

train_df.nunique(axis=0)

"""Dropping similar features:-           
The following groups of features

(extraction_type, extraction_type_group, extraction_type_class),
(payment, payment_type),            
(water_quality, quality_group),         
(source, source_class),     
(subvillage, region, region_code, district_code, lga, ward),         
(waterpoint_type, waterpoint_type_group)       
(scheme_name, scheme_management)    

contain very similar information, so the correlation between them is high. This way we are risking overfitting the training data by including all the features in our analysis.

And num_private is ~99% zeros and has no description


"""

train_df.head(1)

train_df = train_df.drop(['id','installer','wpt_name','num_private','subvillage',
               'region','district_code','lga','ward','scheme_name',
               'extraction_type_group','extraction_type_class','management_group','payment','water_quality',
               'quantity','source_type','waterpoint_type_group','status_group'], axis =1)

train_df.head()

train_df.shape

test_df = test_df.drop(['id','installer','wpt_name','num_private','subvillage',
               'region','district_code','lga','ward','scheme_name',
               'extraction_type_group','extraction_type_class','management_group','payment','water_quality',
               'quantity','source_type','waterpoint_type_group'], axis =1)

test_df.head()

test_df.shape

"""Dealing with missing values"""

train_df.isnull().sum()

test_df.isnull().sum()

# Null scheme_management, funder, public_meeting and permit values replace with "unknown" text
train_df["scheme_management"].fillna("unknown", inplace = True)
train_df["public_meeting"].fillna("unknown", inplace = True)
train_df["permit"].fillna("unknown", inplace = True)
train_df["funder"].fillna("unknown", inplace = True)

test_df["scheme_management"].fillna("unknown", inplace = True)
test_df["public_meeting"].fillna("unknown", inplace = True)
test_df["permit"].fillna("unknown", inplace = True)
test_df["funder"].fillna("unknown", inplace = True)

train_df.isnull().sum().sort_values(ascending = False)

test_df.isnull().sum().sort_values(ascending = False)

"""Reducing cardinality

selecting features
"""

# Get number of unique entries in each column with categorical data

cat_vars = train_df.select_dtypes(include='object').columns
object_nunique = list(map(lambda col: train_df[col].nunique(), cat_vars))
d = dict(zip(cat_vars, object_nunique))

# Print number of unique entries by column, in ascending order
sorted(d.items(), key=lambda x: x[1])

"""Based on the above analysis:

The "recorded_by" feature can be dropped as there is only 1 unique value, it doesn't help in predicting.

The columns in which values can be ordered we can perform an Ordinal encoding:

quality_group   
quantity_group   
payment_type

The cardinality of the following 2 features should be reduced to 10 and then one-hot encode them or try Binary encoding:

scheme_managenemt   
extraction_type   

The cardinality of "funder" is too high, will drop at the first model run, later could try Frequency encoding,Binary encoding if reduce cardinality to at least 100.

The rest can be one-hot encoded as the cardinality is lower than 10:

public_meeting # later -> Binary?    
permit # later-> Binary?   
source_class   
management_group    
waterpoint_type_group    
source_type    
basin
"""

train_df = train_df.drop('recorded_by', axis = 1)

test_df = test_df.drop('recorded_by', axis = 1)

"""Ordinal encoding of categerical data:-

Quality_group
"""

train_df.quality_group.value_counts()

order_dict_quality = {"good":3,"salty":2,"milky":2,"colored":2,"fluoride":2,"unknown":1}
train_df["quality_group_code"] = [order_dict_quality[item] for item in train_df.quality_group]
del train_df["quality_group"]

test_df["quality_group_code"] = [order_dict_quality[item] for item in test_df.quality_group]
del test_df["quality_group"]

"""Quantity_group"""

train_df.quantity_group.value_counts()

order_dict_quantity = {"enough":3,"insufficient":2,"dry":2,"seasonal":2,"unknown":1}
train_df["quantity_group_code"] = [order_dict_quantity[item] for item in train_df.quantity_group] 
del train_df["quantity_group"]

test_df["quantity_group_code"] = [order_dict_quantity[item] for item in test_df.quantity_group] 
del test_df["quantity_group"]

"""Payment_type"""

train_df.payment_type.value_counts()

order_dict_payment = {"monthly":4,"annually":4,"on failure":3,"per bucket":3,"never pay":2,"unknown":1,"other":1}
train_df["payment_code"] = [order_dict_payment[item] for item in train_df.payment_type] 
del train_df["payment_type"]

test_df["payment_code"] = [order_dict_payment[item] for item in test_df.payment_type] 
del test_df["payment_type"]

"""Public_meeting"""

train_df.public_meeting.value_counts()

order_dict_pub_meet = {True:2,False:1,"unknown":0}
train_df["public_meeting_code"] = [order_dict_pub_meet[item] for item in train_df.public_meeting] 
del train_df["public_meeting"]

test_df["public_meeting_code"] = [order_dict_pub_meet[item] for item in test_df.public_meeting] 
del test_df["public_meeting"]

"""Permit"""

train_df.permit.value_counts()

order_dict_permit = {True:2,False:1,"unknown":0}
train_df["permit_code"] = [order_dict_pub_meet[item] for item in train_df.permit] 
del train_df["permit"]

test_df["permit_code"] = [order_dict_pub_meet[item] for item in test_df.permit] 
del test_df["permit"]

"""One hot encoding

Scheme_managment
"""

train_df.scheme_management.value_counts()

def scheme_wrangler(row):
    if row['scheme_management']=='VWC':
        return 'vwc'
    elif row['scheme_management']=='WUG':
        return 'wug'
    elif row['scheme_management']=='Water authority':
        return 'wtr_auth'
    elif row['scheme_management']=='WUA':
        return 'wua'
    elif row['scheme_management']=='Water Board':
        return 'wtr_brd'
    elif row['scheme_management']=='Parastatal':
        return 'parastatal'
    elif row['scheme_management']=='Private operator':
        return 'pri_optr'
    elif row['scheme_management']=='SWC':
        return 'swc'
    elif row['scheme_management']=='Company':
        return 'company'
    else:
        return 'other'
train_df['scheme_management'] = train_df.apply(lambda row: scheme_wrangler(row), axis=1)
test_df['scheme_management'] = test_df.apply(lambda row: scheme_wrangler(row), axis=1)

"""Extraction_type"""

train_df.extraction_type.value_counts()

def extraction_wrangler(row):
    if row['extraction_type']=='gravity':
        return 'gravity'
    elif row['extraction_type']=='nira/tanira':
        return 'nira/tanira'
    elif row['extraction_type']=='submersible':
        return 'submersible'
    elif row['extraction_type']=='swn 80':
        return 'swn_80'
    elif row['extraction_type']=='mono':
        return 'mono'
    elif row['extraction_type']=='india mark ii':
        return 'india_mark_ii'
    elif row['extraction_type']=='afridev':
        return 'afridev'
    elif row['extraction_type']=='ksb':
        return 'ksb'
    elif row['extraction_type']=='windmill':
        return 'windmill'
    else:
        return 'other'
train_df['extraction_type'] = train_df.apply(lambda row: extraction_wrangler(row), axis=1)
test_df['extraction_type'] = test_df.apply(lambda row: extraction_wrangler(row), axis=1)

"""Funder"""

train_df.funder.value_counts()

def funder_wrangler(row):  
    '''Keep top 8 values and set the rest to 'other'''

    if row['funder']=='Government Of Tanzania':
        return 'gov'
    elif row['funder']=='Danida':
        return 'danida'
    elif row['funder']=='Hesawa':
        return 'hesawa'
    elif row['funder']=='Rwssp':
        return 'rwssp'
    elif row['funder']=='World Bank':
        return 'world_bank'   
    elif row['funder']=='Kkkt':
        return 'kkkt'   
    elif row['funder']=='World Vision':
        return 'world_vision'  
    elif row['funder']=='Unicef':
        return 'unicef'
    else:
        return 'other'
train_df['funder'] = train_df.apply(lambda row: funder_wrangler(row), axis=1)
test_df['funder'] = test_df.apply(lambda row: funder_wrangler(row), axis=1)

"""Feature Engineering

Amount_tsh:-  
Based on the plot amount_tsh vs status_group ,if amount_tsh > 200000 are most likely to be functional
"""

train_df.loc[train_df['amount_tsh'] < 200000, 'amount_tsh'] = 0
train_df.loc[train_df['amount_tsh'] >= 200000, 'amount_tsh'] = 1

test_df.loc[train_df['amount_tsh'] < 200000, 'amount_tsh'] = 0
test_df.loc[train_df['amount_tsh'] >= 200000, 'amount_tsh'] = 1

"""Linear Discriminant Analysis (LDA):-

latitude    
longitude   
gps_height
"""

LDA_cols = ["latitude","longitude","gps_height"]

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

train_df_sc = sc.fit_transform(train_df[LDA_cols])
test_df_sc = sc.transform(test_df[LDA_cols])

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=None)

train_df_lda = lda.fit_transform(train_df_sc, train_df.label.values.ravel())
test_df_lda = lda.transform(test_df_sc)

train_df = pd.concat((pd.DataFrame(train_df_lda), train_df), axis=1)
test_df = pd.concat((pd.DataFrame(test_df_lda), test_df), axis=1)

for i in LDA_cols:
    del train_df[i]
    del test_df[i]

train_df.rename(columns={0: "LDA_0",1:"LDA_1"},inplace=True)
test_df.rename(columns={0: "LDA_0",1:"LDA_1"},inplace=True)

"""Construction year:-    
We will turn construction_year into a categorical column with bins containing the following values: '60s', '70s', '80s', '90s, '00s', '10s', 'unknown'.
"""

def construction_wrangler(row):
    if row['construction_year'] >= 1960 and row['construction_year'] < 1970:
        return '60s'
    elif row['construction_year'] >= 1970 and row['construction_year'] < 1980:
        return '70s'
    elif row['construction_year'] >= 1980 and row['construction_year'] < 1990:
        return '80s'
    elif row['construction_year'] >= 1990 and row['construction_year'] < 2000:
        return '90s'
    elif row['construction_year'] >= 2000 and row['construction_year'] < 2010:
        return '00s'
    elif row['construction_year'] >= 2010:
        return '10s'
    else:
        return 'unknown'
train_df['construction_year'] = train_df.apply(lambda row: construction_wrangler(row), axis=1)
test_df['construction_year'] = test_df.apply(lambda row: construction_wrangler(row), axis=1)

""" Date_recorded:-    
 We will calculate the number of days past since the date_recorded of a particular pump till the most recent date of the dataset. The idea being that more recently recorded pumps might be more likely to be functional than non-functional.


"""

train_df.date_recorded = pd.to_datetime(train_df.date_recorded)
test_df.date_recorded = pd.to_datetime(test_df.date_recorded)

train_df.date_recorded.describe()

"""The most recent data is 2013-12-03. Subtract each date from this point to obtain a 'days_since_recorded' column."""

train_df['days_since_recorded'] = pd.datetime(2013, 12, 3) - pd.to_datetime(train_df.date_recorded)
train_df['days_since_recorded'] = train_df['days_since_recorded'].astype('timedelta64[D]').astype(int)

test_df['days_since_recorded'] = pd.datetime(2013, 12, 3) - pd.to_datetime(test_df.date_recorded)
test_df['days_since_recorded'] = test_df['days_since_recorded'].astype('timedelta64[D]').astype(int)

train_df['days_since_recorded']

train_df = train_df.drop("date_recorded",axis=1)
test_df = test_df.drop("date_recorded",axis=1)

train_df.head()

train_df.shape

test_df.head()

test_df.columns

test_df = test_df.drop('Unnamed: 0', axis = 1)

test_df.shape

"""One-hot encoding of categorical features"""

cat_vars = train_df.select_dtypes(include='object').columns
print(cat_vars)
len(cat_vars)

from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_df[cat_vars])).astype(np.int64)
OH_cols_test = pd.DataFrame(OH_encoder.transform(test_df[cat_vars])).astype(np.int64)

# One-hot encoding removed index; put it back
OH_cols_train.index = train_df.index
OH_cols_test.index = test_df.index

OH_cols_train.columns = OH_encoder.get_feature_names(cat_vars)
OH_cols_test.columns = OH_encoder.get_feature_names(cat_vars)

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = train_df.drop(cat_vars, axis=1)
num_X_valid = test_df.drop(cat_vars, axis=1)

# Add one-hot encoded columns to numerical features
OH_train_df = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_test_df = pd.concat([num_X_valid, OH_cols_test], axis=1)

OH_train_df.head()

"""3) Declaration of ML frameworks (e.g. SVM with Gaussian kernel) to be used with a prior
hypothesis of which method is likely to work better (it does not matter if your initial
hypothesis turns out to be wrong) due to insights from the previous steps. Declare some
resources on the net that you read to find out which ML framework is better suited for
which type of data.

L1-regularization with Logistic regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel

X, y = OH_train_df[OH_train_df.columns.drop("label")], OH_train_df['label']

# Set the regularization parameter C=1

logistic = LogisticRegression(solver="saga",C=1, penalty="l1", random_state=7).fit(X, y)
model = SelectFromModel(logistic, prefit=True)

X_new = model.transform(X)
X_new

# Get back the kept features as a DataFrame with dropped columns as all 0s

selected_features = pd.DataFrame(model.inverse_transform(X_new), 
                                 index=X.index,
                                 columns=X.columns)

# Dropped columns have values of all 0s, keep other columns 
selected_columns = selected_features.columns[selected_features.var() != 0]

len(selected_columns)

selected_columns

train_df_selected_features = OH_train_df[selected_columns].join(y)

test_df_selected_features = OH_test_df[selected_columns]

train_df_selected_features.shape

train_df_selected_features.to_csv("train_df_final.csv", index=False)
test_df_selected_features.to_csv("test_df_final.csv", index=False)

"""4) Diligent hyperparameter tuning for at least three frameworks."""

#Importing required labraries for model selection

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler as ss
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

#Trees    
from sklearn.tree import DecisionTreeClassifier

#Ensemble
from sklearn.ensemble import RandomForestClassifier
import xgboost
from xgboost import XGBClassifier

#SVM
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.svm import NuSVC

# metrics
from sklearn.metrics import accuracy_score, confusion_matrix

# PCA
from sklearn import decomposition

train_df_final = pd.read_csv("/content/train_df_final.csv")
X_test_final = pd.read_csv("/content/test_df_final.csv")

X_test_final.shape

train_df_final.shape

"""Train/Test splitting"""

X = train_df_final.drop("label",axis=1)
y = train_df_final["label"]

# Create training and test sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=42)

X.isnull().values.any()

"""Standard Scaling:-   
The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1.
"""

sc = ss()
X_train = sc.fit_transform(X_train)
X_valid = sc.transform(X_valid)
X_test = sc.transform(X_test_final)

"""Model selection"""

# Decision Tree

decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)
y_pred = decision_tree.predict(X_valid)

acc_decision_tree = round(accuracy_score(y_valid,y_pred) * 100, 2)
acc_decision_tree

# Random Forest

rfc = RandomForestClassifier(criterion='entropy', n_estimators = 1000,min_samples_split=8,random_state=42,verbose=5)
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_valid)

acc_rfc = round(accuracy_score(y_valid,y_pred) * 100, 2)
acc_rfc

# XGBoost

xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=5)
xgb.fit(X_train, y_train, 
             early_stopping_rounds=5, 
             eval_set=[(X_valid, y_valid)], 
             verbose=False)

y_pred = xgb.predict(X_valid)
acc_xgb = round(accuracy_score(y_valid,y_pred) * 100, 2)
acc_xgb

# Linear SVC

linear_SVC = LinearSVC()
linear_SVC.fit(X_train,y_train)
linear_SVC.predict(X_valid)

acc_linear_SVC = round(accuracy_score(y_valid,y_pred) * 100, 2)
acc_linear_SVC

"""Compare model results"""

models = pd.DataFrame({
    'Model': ['Decision Tree','Random Forest','Linear SVC', 
              "XGBoost"],
    'Score': [acc_decision_tree,acc_rfc, acc_linear_SVC, 
              acc_xgb]})
sorted_by_score = models.sort_values(by='Score', ascending=False)

#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html
sns.barplot(x='Score', y = 'Model', data = sorted_by_score, color = 'g')

#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html
plt.title('Machine Learning Algorithm Accuracy Score \n')
plt.xlabel('Accuracy Score on validation data (%)')
plt.ylabel('Model')

"""Among the 4 models we have best accuracy for Random Forest

5) Determination of the relative order of importance of variables, and perhaps a second
round of feature selection.

Feature importance with random forest
"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(criterion='gini',min_samples_split=8, n_estimators=1000,
                           random_state = 7)
rf.fit(X, y)

# helper function for creating a feature importance dataframe

def imp_df(column_names, importances):
    df = pd.DataFrame({'feature': column_names,
                       'feature_importance': importances}) \
           .sort_values('feature_importance', ascending = False) \
           .reset_index(drop = True)
    return df

# plotting a feature importance dataframe (horizontal barchart)

def var_imp_plot(imp_df, title):
    imp_df.columns = ['feature', 'feature_importance']
    sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \
       .set_title(title, fontsize = 20)

base_imp = imp_df(X.columns, rf.feature_importances_)
top_30_imp = base_imp[0:30]
top_30_features = top_30_imp.feature

pylab.rcParams["figure.figsize"] = (50,50)

var_imp_plot(base_imp, 'Default feature importance (scikit-learn)')

train_df_final_top_imp = OH_train_df[top_30_features].join(y)
test_df_final_top_imp = OH_test_df[top_30_features]

train_df_final_top_imp.shape

"""7) Submit the final labels (you should not touch the test data before this step) as a CSV file
with a single column and no header.
"""

submission_df = pd.DataFrame()

X_test = sc.transform(X_test_final)
submission_df['status_group']=rfc.predict(X_test)

vals_to_replace = {2:'functional', 1:'functional needs repair', 0:'non functional'}

submission_df.status_group = submission_df.status_group.replace(vals_to_replace)

submission_df.to_csv("submission_TatianaSwrt_rfc_noretrain_80.csv",sep=',', index=False)

data = pd.read_csv('/content/submission_TatianaSwrt_rfc_noretrain_80.csv')

data.head()

data.shape

"""8. Make the notebook easy and insightful to read and declare inspiration sources.

References:-

https://www.kaggle.com
"""